{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFQPUMWzXPsC"
      },
      "outputs": [],
      "source": [
        "import os, sys, glob, shutil, subprocess, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (accuracy_score, precision_recall_fscore_support,\n",
        "                             confusion_matrix, roc_curve, auc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqhS3QBXXTSQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED); tf.random.set_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApROXYXdpisy",
        "outputId": "9a9d143e-bc71-4463-ea31-26198efdb836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> apt-get -y update && apt-get -y install p7zip-full\n",
            "> pip -q install patool lime scikit-image\n"
          ]
        }
      ],
      "source": [
        "def _run(cmd):\n",
        "    print(\">\", cmd)\n",
        "    return subprocess.run(cmd, shell=True, check=False, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    _run(\"apt-get -y update && apt-get -y install p7zip-full\")\n",
        "    _run(\"pip -q install patool lime scikit-image\")\n",
        "\n",
        "import patoolib\n",
        "from skimage.segmentation import slic\n",
        "from lime import lime_image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tjh8matXp69S"
      },
      "outputs": [],
      "source": [
        "YES_RAR_PATH = \"/content/yes.rar\"\n",
        "NO_RAR_PATH  = \"/content/no.rar\"\n",
        "ROOT_DIR = \"/content/data\"\n",
        "YES_DIR  = os.path.join(ROOT_DIR, \"yes\")\n",
        "NO_DIR   = os.path.join(ROOT_DIR, \"no\")\n",
        "\n",
        "IMG_SIZE = (300, 300)       # ↑ bigger input improves EfficientNet performance\n",
        "BATCH_SIZE = 16\n",
        "VAL_SPLIT = 0.2\n",
        "TEST_SPLIT = 0.1\n",
        "\n",
        "WARMUP_EPOCHS =30\n",
        "FINETUNE_EPOCHS = 12        # total epochs = WARMUP + FINETUNE\n",
        "UNFREEZE_AT = 200           # unfreeze last N layers in fine-tune\n",
        "LR_WARMUP = 1e-3\n",
        "LR_FINETUNE = 3e-5\n",
        "WEIGHT_DECAY = 1e-5         # AdamW\n",
        "\n",
        "TTA_N = 5                   # TTA passes at inference\n",
        "F_BETA = 1.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yB5F5lkqELO"
      },
      "outputs": [],
      "source": [
        "def ensure_dirs():\n",
        "    os.makedirs(ROOT_DIR, exist_ok=True)\n",
        "    os.makedirs(YES_DIR, exist_ok=True)\n",
        "    os.makedirs(NO_DIR, exist_ok=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXxEyRj8qVA4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def dir_is_empty(path):\n",
        "    return (not os.path.exists(path)) or (len(os.listdir(path)) == 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbxzOFb6qvWj"
      },
      "outputs": [],
      "source": [
        "def extract_rar_if_needed(rar_path, outdir):\n",
        "    if os.path.isfile(rar_path) and dir_is_empty(outdir):\n",
        "        print(f\"Extracting {rar_path} -> {outdir}\")\n",
        "        try:\n",
        "            patoolib.extract_archive(rar_path, outdir=outdir)\n",
        "        except Exception as e:\n",
        "            print(f\"Extraction failed for {rar_path}: {e}\")\n",
        "    else:\n",
        "        if not os.path.isfile(rar_path):\n",
        "            print(f\"(Info) {rar_path} not found. Skipping extraction.\")\n",
        "        else:\n",
        "            print(f\"(Info) {outdir} already has files. Skipping extraction.\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XniIR_Yq94S"
      },
      "outputs": [],
      "source": [
        "def flatten_images_inplace(root):\n",
        "    exts = {\"jpg\",\"jpeg\",\"png\",\"bmp\"}\n",
        "    moved = 0\n",
        "    for dirpath, _, filenames in os.walk(root):\n",
        "        if dirpath == root: continue\n",
        "        for fn in filenames:\n",
        "            if fn.lower().split(\".\")[-1] in exts:\n",
        "                src = os.path.join(dirpath, fn)\n",
        "                dst = os.path.join(root, fn)\n",
        "                base, ext2 = os.path.splitext(dst); k = 1\n",
        "                while os.path.exists(dst):\n",
        "                    dst = f\"{base}_{k}{ext2}\"; k += 1\n",
        "                shutil.move(src, dst); moved += 1\n",
        "    for dirpath, _, _ in os.walk(root, topdown=False):\n",
        "        if dirpath != root and len(os.listdir(dirpath)) == 0:\n",
        "            shutil.rmtree(dirpath, ignore_errors=True)\n",
        "    if moved: print(f\"(Info) Flattened {moved} images into {root}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfC7egterAqD",
        "outputId": "728a335c-e706-434d-8376-e3ddfb581ff8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO patool: Extracting /content/yes.rar ...\n",
            "INFO:patool:Extracting /content/yes.rar ...\n",
            "INFO patool: running /usr/bin/unrar x -kb -or -- /content/yes.rar\n",
            "INFO:patool:running /usr/bin/unrar x -kb -or -- /content/yes.rar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/yes.rar -> /content/data/yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO patool: ... /content/yes.rar extracted to `/content/data/yes'.\n",
            "INFO:patool:... /content/yes.rar extracted to `/content/data/yes'.\n",
            "INFO patool: Extracting /content/no.rar ...\n",
            "INFO:patool:Extracting /content/no.rar ...\n",
            "INFO patool: running /usr/bin/unrar x -kb -or -- /content/no.rar\n",
            "INFO:patool:running /usr/bin/unrar x -kb -or -- /content/no.rar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/no.rar -> /content/data/no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO patool: ... /content/no.rar extracted to `/content/data/no'.\n",
            "INFO:patool:... /content/no.rar extracted to `/content/data/no'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Info) Flattened 1500 images into /content/data/yes\n",
            "(Info) Flattened 1500 images into /content/data/no\n"
          ]
        }
      ],
      "source": [
        "ensure_dirs()\n",
        "extract_rar_if_needed(YES_RAR_PATH, YES_DIR)\n",
        "extract_rar_if_needed(NO_RAR_PATH, NO_DIR)\n",
        "flatten_images_inplace(YES_DIR)\n",
        "flatten_images_inplace(NO_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhxWUHCfrP4Y",
        "outputId": "f9f4ac02-3a82-44a6-9c14-e511115e3170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== DATASET SUMMARY (Before Split) ===\n",
            "no: 1500\n",
            "yes: 1500\n",
            "TOTAL: 3000\n"
          ]
        }
      ],
      "source": [
        "def list_images_labels(root_dir):\n",
        "    classes = [d for d in sorted(os.listdir(root_dir)) if os.path.isdir(os.path.join(root_dir, d))]\n",
        "    assert len(classes) == 2, f\"Expected 2 classes. Found: {classes}\"\n",
        "    def gather(c):\n",
        "        fps = sorted(glob.glob(os.path.join(root_dir, c, \"**\", \"*\"), recursive=True))\n",
        "        return [f for f in fps if os.path.isfile(f) and f.lower().split(\".\")[-1] in {\"jpg\",\"jpeg\",\"png\",\"bmp\"}]\n",
        "    files0 = gather(classes[0]); files1 = gather(classes[1])\n",
        "    X = np.array(files0 + files1)\n",
        "    y = np.array([0]*len(files0) + [1]*len(files1), dtype=int)\n",
        "\n",
        "    print(\"\\n=== DATASET SUMMARY (Before Split) ===\")\n",
        "    print(f\"{classes[0]}: {len(files0)}\")\n",
        "    print(f\"{classes[1]}: {len(files1)}\")\n",
        "    print(f\"TOTAL: {len(X)}\")\n",
        "    return classes, X, y\n",
        "\n",
        "classes, X_all, y_all = list_images_labels(ROOT_DIR)\n",
        "TOTAL = len(X_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CatJZhtUrU7S"
      },
      "outputs": [],
      "source": [
        "def make_splits(X, y, val_split=0.2, test_split=0.1, seed=SEED):\n",
        "    X_tmp, X_test, y_tmp, y_test = train_test_split(X, y, test_size=test_split, random_state=seed, stratify=y)\n",
        "    val_rel = val_split / (1. - test_split)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_tmp, y_tmp, test_size=val_rel, random_state=seed, stratify=y_tmp)\n",
        "    return (X_train,y_train), (X_val,y_val), (X_test,y_test)\n",
        "\n",
        "(X_train, y_train), (X_val, y_val), (X_test, y_test) = make_splits(X_all, y_all, VAL_SPLIT, TEST_SPLIT)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK-NEm0Zr7fI",
        "outputId": "dda01a0c-e711-44d8-cf41-438c01ebe041"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== SPLIT SUMMARY ===\n",
            "Train: 2100 {'no': 1050, 'yes': 1050}\n",
            "Val:   600 {'no': 300, 'yes': 300}\n",
            "Test:  300 {'no': 150, 'yes': 150}\n",
            "Sum(Train+Val+Test): 3000 | TOTAL: 3000\n"
          ]
        }
      ],
      "source": [
        "def split_report():\n",
        "    def cnt(y): return {classes[i]: int((y==i).sum()) for i in range(2)}\n",
        "    print(\"\\n=== SPLIT SUMMARY ===\")\n",
        "    print(\"Train:\", len(X_train), cnt(y_train))\n",
        "    print(\"Val:  \", len(X_val),   cnt(y_val))\n",
        "    print(\"Test: \", len(X_test),  cnt(y_test))\n",
        "    print(\"Sum(Train+Val+Test):\", len(X_train)+len(X_val)+len(X_test), \"| TOTAL:\", TOTAL)\n",
        "    assert len(X_train)+len(X_val)+len(X_test) == TOTAL\n",
        "\n",
        "split_report()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOpXfzBmsCvQ"
      },
      "outputs": [],
      "source": [
        "def decode_resize_uint8(path, label, img_size=IMG_SIZE):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.io.decode_image(img, channels=3, expand_animations=False)\n",
        "    img = tf.image.resize(img, img_size)                 # float32 0..255 if not cast yet\n",
        "    img = tf.cast(img, tf.float32)                       # keep 0..255\n",
        "    return img, tf.cast(label, tf.int32)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "et4cylPisLge"
      },
      "outputs": [],
      "source": [
        "def make_tfds(X, y, shuffle=False, seed=SEED, batch_size=BATCH_SIZE):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(X), seed=seed, reshuffle_each_iteration=False)\n",
        "    ds = ds.map(lambda a,b: decode_resize_uint8(a,b,IMG_SIZE), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.batch(batch_size, drop_remainder=False).prefetch(tf.data.AUTOTUNE)\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ij1d0TR1sTUG"
      },
      "outputs": [],
      "source": [
        "train_ds = make_tfds(X_train, y_train, shuffle=True)\n",
        "val_ds   = make_tfds(X_val,   y_val,   shuffle=False)\n",
        "test_ds  = make_tfds(X_test,  y_test,  shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd05IY7rsZqr",
        "outputId": "05f2f2e1-c10f-4a38-e676-18a31efc589b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== PIPELINE COUNTS ===\n",
            "Train DS: 2100  Val DS: 600  Test DS: 300  Total: 3000\n"
          ]
        }
      ],
      "source": [
        "def count_ds(ds):\n",
        "    s=0\n",
        "    for a,b in ds: s += a.shape[0]\n",
        "    return int(s)\n",
        "print(\"\\n=== PIPELINE COUNTS ===\")\n",
        "print(\"Train DS:\", count_ds(train_ds), \" Val DS:\", count_ds(val_ds), \" Test DS:\", count_ds(test_ds), \" Total:\", TOTAL)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HiOYo-Opkad",
        "outputId": "63938bd0-8752-4c24-d1a4-b55101a553b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Class weights: {0: 0.9999999999995238, 1: 0.9999999999995238}\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "ctr = Counter(y_train.tolist())\n",
        "w0 = (len(y_train) / (2.0 * ctr[0] + 1e-9))\n",
        "w1 = (len(y_train) / (2.0 * ctr[1] + 1e-9))\n",
        "class_weight = {0: w0, 1: w1}\n",
        "print(\"\\nClass weights:\", class_weight)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "btDkazZcscVy",
        "outputId": "e94f3287-81a9-4819-afcb-001a5854e557"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"EffB0_binary\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"EffB0_binary\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_uint8 (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ augment (\u001b[38;5;33mSequential\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m1280\u001b[0m)   │     \u001b[38;5;34m4,049,571\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m1,281\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_uint8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ augment (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,050,852\u001b[0m (15.45 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,050,852</span> (15.45 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,281\u001b[0m (5.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> (5.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,049,571\u001b[0m (15.45 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> (15.45 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "Aug = keras.Sequential([\n",
        "    keras.layers.RandomFlip(\"horizontal\"),                # brain MRIs: LR flip ok, avoid UD flip\n",
        "    keras.layers.RandomRotation(0.1),\n",
        "    keras.layers.RandomZoom(0.1),\n",
        "    keras.layers.RandomContrast(0.1),\n",
        "], name=\"augment\")\n",
        "\n",
        "inputs = keras.Input(shape=IMG_SIZE+(3,), name=\"input_uint8\")\n",
        "x = Aug(inputs)\n",
        "x = tf.keras.applications.efficientnet.preprocess_input(x)   # handles 0..255 -> normalized\n",
        "base = tf.keras.applications.EfficientNetB0(include_top=False, input_shape=IMG_SIZE+(3,), weights=\"imagenet\")\n",
        "base.trainable = False\n",
        "x = base(x, training=False)\n",
        "x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = keras.layers.Dropout(0.3)(x)\n",
        "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs, name=\"EffB0_binary\")\n",
        "\n",
        "# AdamW + weight decay\n",
        "optimizer = keras.optimizers.AdamW(learning_rate=LR_WARMUP, weight_decay=WEIGHT_DECAY)\n",
        "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True),\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.3, patience=2, min_lr=1e-6, verbose=1),\n",
        "    keras.callbacks.ModelCheckpoint(\"/content/best_warmup.keras\", monitor=\"val_loss\", save_best_only=True, verbose=1),\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2g_cuEx6sgRl",
        "outputId": "36ce9226-13df-4684-98c7-a3a4f85128f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.6747 - loss: 0.5907\n",
            "Epoch 1: val_loss improved from inf to 0.41864, saving model to /content/best_warmup.keras\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m518s\u001b[0m 4s/step - accuracy: 0.6752 - loss: 0.5901 - val_accuracy: 0.8333 - val_loss: 0.4186 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8154 - loss: 0.4143\n",
            "Epoch 2: val_loss improved from 0.41864 to 0.34511, saving model to /content/best_warmup.keras\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m370s\u001b[0m 3s/step - accuracy: 0.8156 - loss: 0.4141 - val_accuracy: 0.8733 - val_loss: 0.3451 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8576 - loss: 0.3594\n",
            "Epoch 3: val_loss improved from 0.34511 to 0.30536, saving model to /content/best_warmup.keras\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 3s/step - accuracy: 0.8577 - loss: 0.3593 - val_accuracy: 0.9067 - val_loss: 0.3054 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8762 - loss: 0.3130\n",
            "Epoch 4: val_loss improved from 0.30536 to 0.27771, saving model to /content/best_warmup.keras\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m391s\u001b[0m 3s/step - accuracy: 0.8763 - loss: 0.3128 - val_accuracy: 0.9083 - val_loss: 0.2777 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8750 - loss: 0.2972\n",
            "Epoch 5: val_loss improved from 0.27771 to 0.26069, saving model to /content/best_warmup.keras\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 3s/step - accuracy: 0.8751 - loss: 0.2971 - val_accuracy: 0.9183 - val_loss: 0.2607 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8802 - loss: 0.2802\n",
            "Epoch 6: val_loss improved from 0.26069 to 0.23823, saving model to /content/best_warmup.keras\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 3s/step - accuracy: 0.8802 - loss: 0.2802 - val_accuracy: 0.9317 - val_loss: 0.2382 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9051 - loss: 0.2526\n",
            "Epoch 7: val_loss improved from 0.23823 to 0.22410, saving model to /content/best_warmup.keras\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m360s\u001b[0m 3s/step - accuracy: 0.9051 - loss: 0.2526 - val_accuracy: 0.9317 - val_loss: 0.2241 - learning_rate: 0.0010\n",
            "Epoch 8/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9013 - loss: 0.2447\n",
            "Epoch 8: val_loss improved from 0.22410 to 0.22214, saving model to /content/best_warmup.keras\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 3s/step - accuracy: 0.9014 - loss: 0.2446 - val_accuracy: 0.9300 - val_loss: 0.2221 - learning_rate: 0.0010\n",
            "Epoch 9/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8994 - loss: 0.2475\n",
            "Epoch 9: val_loss improved from 0.22214 to 0.21662, saving model to /content/best_warmup.keras\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 3s/step - accuracy: 0.8995 - loss: 0.2474 - val_accuracy: 0.9317 - val_loss: 0.2166 - learning_rate: 0.0010\n",
            "Epoch 10/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9182 - loss: 0.2270\n",
            "Epoch 10: val_loss improved from 0.21662 to 0.21049, saving model to /content/best_warmup.keras\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m350s\u001b[0m 3s/step - accuracy: 0.9182 - loss: 0.2269 - val_accuracy: 0.9367 - val_loss: 0.2105 - learning_rate: 0.0010\n",
            "Epoch 11/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9168 - loss: 0.2233\n",
            "Epoch 11: val_loss improved from 0.21049 to 0.19517, saving model to /content/best_warmup.keras\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 3s/step - accuracy: 0.9168 - loss: 0.2233 - val_accuracy: 0.9400 - val_loss: 0.1952 - learning_rate: 0.0010\n",
            "Epoch 12/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9182 - loss: 0.2246\n",
            "Epoch 12: val_loss improved from 0.19517 to 0.19444, saving model to /content/best_warmup.keras\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m347s\u001b[0m 3s/step - accuracy: 0.9182 - loss: 0.2245 - val_accuracy: 0.9417 - val_loss: 0.1944 - learning_rate: 0.0010\n",
            "Epoch 13/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9160 - loss: 0.2076\n",
            "Epoch 13: val_loss improved from 0.19444 to 0.19190, saving model to /content/best_warmup.keras\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 3s/step - accuracy: 0.9161 - loss: 0.2076 - val_accuracy: 0.9433 - val_loss: 0.1919 - learning_rate: 0.0010\n",
            "Epoch 14/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9179 - loss: 0.2208\n",
            "Epoch 14: val_loss did not improve from 0.19190\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m360s\u001b[0m 3s/step - accuracy: 0.9180 - loss: 0.2208 - val_accuracy: 0.9283 - val_loss: 0.2130 - learning_rate: 0.0010\n",
            "Epoch 15/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9219 - loss: 0.2041\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "\n",
            "Epoch 15: val_loss did not improve from 0.19190\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m366s\u001b[0m 3s/step - accuracy: 0.9219 - loss: 0.2040 - val_accuracy: 0.9417 - val_loss: 0.1923 - learning_rate: 0.0010\n",
            "Epoch 16/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9201 - loss: 0.2126\n",
            "Epoch 16: val_loss improved from 0.19190 to 0.18806, saving model to /content/best_warmup.keras\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 3s/step - accuracy: 0.9202 - loss: 0.2125 - val_accuracy: 0.9417 - val_loss: 0.1881 - learning_rate: 3.0000e-04\n",
            "Epoch 17/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9245 - loss: 0.2127\n",
            "Epoch 17: val_loss improved from 0.18806 to 0.17801, saving model to /content/best_warmup.keras\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m359s\u001b[0m 3s/step - accuracy: 0.9246 - loss: 0.2125 - val_accuracy: 0.9467 - val_loss: 0.1780 - learning_rate: 3.0000e-04\n",
            "Epoch 18/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9299 - loss: 0.1953\n",
            "Epoch 18: val_loss did not improve from 0.17801\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 3s/step - accuracy: 0.9299 - loss: 0.1953 - val_accuracy: 0.9433 - val_loss: 0.1813 - learning_rate: 3.0000e-04\n",
            "Epoch 19/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9332 - loss: 0.1981\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "\n",
            "Epoch 19: val_loss did not improve from 0.17801\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 3s/step - accuracy: 0.9332 - loss: 0.1981 - val_accuracy: 0.9417 - val_loss: 0.1870 - learning_rate: 3.0000e-04\n",
            "Epoch 20/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9417 - loss: 0.1778\n",
            "Epoch 20: val_loss did not improve from 0.17801\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 3s/step - accuracy: 0.9417 - loss: 0.1778 - val_accuracy: 0.9417 - val_loss: 0.1848 - learning_rate: 9.0000e-05\n",
            "Epoch 21/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9307 - loss: 0.1905\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "\n",
            "Epoch 21: val_loss did not improve from 0.17801\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m359s\u001b[0m 3s/step - accuracy: 0.9307 - loss: 0.1904 - val_accuracy: 0.9417 - val_loss: 0.1861 - learning_rate: 9.0000e-05\n",
            "Epoch 22/30\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9335 - loss: 0.1952\n",
            "Epoch 22: val_loss did not improve from 0.17801\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 3s/step - accuracy: 0.9335 - loss: 0.1952 - val_accuracy: 0.9417 - val_loss: 0.1860 - learning_rate: 2.7000e-05\n"
          ]
        }
      ],
      "source": [
        "hist_warm = model.fit(train_ds, validation_data=val_ds, epochs=WARMUP_EPOCHS,\n",
        "                      class_weight=class_weight, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iskwiHMqsnS0"
      },
      "outputs": [],
      "source": [
        "base.trainable = True\n",
        "for layer in base.layers[:-UNFREEZE_AT]:\n",
        "    layer.trainable = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eifYzRYRp9PI"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=keras.optimizers.AdamW(learning_rate=LR_FINETUNE, weight_decay=WEIGHT_DECAY),\n",
        "              loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jizrzI-AqDSo"
      },
      "outputs": [],
      "source": [
        "callbacks_ft = [\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True),\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.3, patience=2, min_lr=1e-6, verbose=1),\n",
        "    keras.callbacks.ModelCheckpoint(\"/content/best_finetune.keras\", monitor=\"val_loss\", save_best_only=True, verbose=1),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv45MB3pqGYQ",
        "outputId": "a0bfa421-9a60-459d-8224-eb5bd3c44016"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.7781 - loss: 0.4764\n",
            "Epoch 1: val_loss improved from inf to 0.16138, saving model to /content/best_finetune.keras\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m856s\u001b[0m 6s/step - accuracy: 0.7786 - loss: 0.4755 - val_accuracy: 0.9317 - val_loss: 0.1614 - learning_rate: 3.0000e-05\n",
            "Epoch 2/12\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.9107 - loss: 0.2157\n",
            "Epoch 2: val_loss improved from 0.16138 to 0.11779, saving model to /content/best_finetune.keras\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m849s\u001b[0m 6s/step - accuracy: 0.9107 - loss: 0.2156 - val_accuracy: 0.9533 - val_loss: 0.1178 - learning_rate: 3.0000e-05\n",
            "Epoch 3/12\n",
            "\u001b[1m 23/132\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9:45\u001b[0m 5s/step - accuracy: 0.9322 - loss: 0.1451"
          ]
        }
      ],
      "source": [
        "hist_ft = model.fit(train_ds, validation_data=val_ds, epochs=FINETUNE_EPOCHS,\n",
        "                    class_weight=class_weight, callbacks=callbacks_ft)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiNoFTigsr8b"
      },
      "outputs": [],
      "source": [
        "def plot_curves(hists, labels):\n",
        "    plt.figure(figsize=(7,4))\n",
        "    for h, lab in zip(hists, labels):\n",
        "        plt.plot(h.history[\"accuracy\"], label=f\"{lab} Train Acc\")\n",
        "        plt.plot(h.history[\"val_accuracy\"], label=f\"{lab} Val Acc\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.title(\"Training vs Validation Accuracy\")\n",
        "    plt.legend(); plt.tight_layout(); plt.show()\n",
        "\n",
        "    plt.figure(figsize=(7,4))\n",
        "    for h, lab in zip(hists, labels):\n",
        "        plt.plot(h.history[\"loss\"], label=f\"{lab} Train Loss\")\n",
        "        plt.plot(h.history[\"val_loss\"], label=f\"{lab} Val Loss\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Training vs Validation Loss\")\n",
        "    plt.legend(); plt.tight_layout(); plt.show()\n",
        "\n",
        "plot_curves([hist_warm, hist_ft], [\"Initial\",\"Finetune\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pinbkvaPsxWl"
      },
      "outputs": [],
      "source": [
        "def collect_preds(dataset, model, tta=False, N=TTA_N):\n",
        "    y_true, y_prob = [], []\n",
        "    if not tta:\n",
        "        for bx, by in dataset:\n",
        "            p = model.predict(bx, verbose=0).reshape(-1)\n",
        "            y_prob.append(p); y_true.append(by.numpy())\n",
        "    else:\n",
        "        # Simple TTA: original + horizontal flip (others can be added)\n",
        "        for bx, by in dataset:\n",
        "            probs_list = []\n",
        "            p0 = model.predict(bx, verbose=0).reshape(-1); probs_list.append(p0)\n",
        "            p1 = model.predict(tf.image.flip_left_right(bx), verbose=0).reshape(-1); probs_list.append(p1)\n",
        "            # Add minor rotations/contrast jitter TTA samples\n",
        "            for _ in range(max(0, N-2)):\n",
        "                bx_aug = tf.image.random_contrast(bx, 0.9, 1.1)\n",
        "                probs_list.append(model.predict(bx_aug, verbose=0).reshape(-1))\n",
        "            p_avg = np.mean(np.stack(probs_list, axis=0), axis=0)\n",
        "            y_prob.append(p_avg); y_true.append(by.numpy())\n",
        "    y_true = np.concatenate(y_true, axis=0)\n",
        "    y_prob = np.concatenate(y_prob, axis=0)\n",
        "    return y_true.astype(int), y_prob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6TnEGS_s0wL"
      },
      "outputs": [],
      "source": [
        "def specificity_from_cm(cm):\n",
        "    TN, FP = cm[0,0], cm[0,1]\n",
        "    return float(TN/(TN+FP)) if (TN+FP)>0 else 0.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBV1iNifs3bt"
      },
      "outputs": [],
      "source": [
        "def bin_metrics(y_true, y_pred):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    spec = specificity_from_cm(cm)\n",
        "    return {\"accuracy\":acc, \"precision\":prec, \"recall/sensitivity\":rec, \"specificity\":spec, \"f1\":f1}, cm\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QNmpdWPs-Un"
      },
      "outputs": [],
      "source": [
        "def bootstrap_ci(y_true, y_pred, n_boot=500, alpha=0.05):\n",
        "    rng = np.random.default_rng(SEED)\n",
        "    N = len(y_true)\n",
        "    keys = [\"accuracy\",\"precision\",\"recall/sensitivity\",\"specificity\",\"f1\"]\n",
        "    store = {k:[] for k in keys}\n",
        "    for _ in range(n_boot):\n",
        "        idx = rng.integers(0, N, N)\n",
        "        m,_ = bin_metrics(y_true[idx], y_pred[idx])\n",
        "        for k in keys: store[k].append(m[k])\n",
        "    ci = {k: (float(np.percentile(v, 2.5)), float(np.percentile(v, 97.5))) for k,v in store.items()}\n",
        "    return ci"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnBnQHjYtBQz"
      },
      "outputs": [],
      "source": [
        "def choose_threshold_by_val(y_true_val, y_prob_val, beta=F_BETA):\n",
        "    # choose thr maximizing F-beta on Validation\n",
        "    best_thr, best_score = 0.5, -1.0\n",
        "    for thr in np.linspace(0.05, 0.95, 181):\n",
        "        y_pred = (y_prob_val >= thr).astype(int)\n",
        "        _, _, f1, _ = precision_recall_fscore_support(y_true_val, y_pred, average=\"binary\", zero_division=0)\n",
        "        # approximate F-beta from precision & recall\n",
        "        prec, rec, _, _ = precision_recall_fscore_support(y_true_val, y_pred, average=\"binary\", zero_division=0)\n",
        "        if prec+rec == 0:\n",
        "            score = 0\n",
        "        else:\n",
        "            score = (1+beta**2) * (prec*rec) / (beta**2 * prec + rec + 1e-12)\n",
        "        if score > best_score:\n",
        "            best_score, best_thr = score, thr\n",
        "    return best_thr, best_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ib1UIrGGtEp_"
      },
      "outputs": [],
      "source": [
        "y_true_val, y_prob_val = collect_preds(val_ds, model, tta=True)\n",
        "y_true_te,  y_prob_te  = collect_preds(test_ds, model, tta=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u43hBYlntG_w"
      },
      "outputs": [],
      "source": [
        "thr, fbeta = choose_threshold_by_val(y_true_val, y_prob_val, beta=F_BETA)\n",
        "print(f\"\\nChosen decision threshold (by Val F{F_BETA}): {thr:.3f}  | best Fβ: {fbeta:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWMS4ri1tMlF"
      },
      "outputs": [],
      "source": [
        "y_true_val, y_prob_val, y_pred_val = collect_preds(val_dataset, model)\n",
        "y_true_te,  y_prob_te,  y_pred_te  = collect_preds(test_dataset, model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZScUCz2YtPSZ"
      },
      "outputs": [],
      "source": [
        "y_pred_val = (y_prob_val >= thr).astype(int)\n",
        "y_pred_te  = (y_prob_te  >= thr).astype(int)\n",
        "\n",
        "m_val, cm_val = bin_metrics(y_true_val, y_pred_val)\n",
        "m_te,  cm_te  = bin_metrics(y_true_te,  y_pred_te)\n",
        "ci_val = bootstrap_ci(y_true_val, y_pred_val)\n",
        "ci_te  = bootstrap_ci(y_true_te,  y_pred_te)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jy1iirWTtSeE"
      },
      "outputs": [],
      "source": [
        "print(\"\\n=== VALIDATION (threshold-tuned) ===\")\n",
        "for k,v in m_val.items():\n",
        "    lo,hi = ci_val[k]; print(f\"{k:>18}: {v:.4f}  (95% CI: {lo:.4f}–{hi:.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ET7GZUCqtU7G"
      },
      "outputs": [],
      "source": [
        "print(\"\\n=== TEST (threshold from Val) ===\")\n",
        "for k,v in m_te.items():\n",
        "    lo,hi = ci_te[k]; print(f\"{k:>18}: {v:.4f}  (95% CI: {lo:.4f}–{hi:.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ryeLEv4tal4"
      },
      "outputs": [],
      "source": [
        "def plot_cm(cm, title):\n",
        "    fig = plt.figure(figsize=(5,4)); ax = fig.add_subplot(111)\n",
        "    im = ax.imshow(cm, interpolation=\"nearest\"); ax.set_title(title)\n",
        "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "    ax.set_xticks([0,1]); ax.set_xticklabels(classes, rotation=45, ha=\"right\")\n",
        "    ax.set_yticks([0,1]); ax.set_yticklabels(classes)\n",
        "    th = cm.max()/2.0\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            ax.text(j, i, int(cm[i,j]), ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i,j] > th else \"black\")\n",
        "    ax.set_ylabel(\"True\"); ax.set_xlabel(\"Pred\"); plt.tight_layout(); plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiGgx95eteY_"
      },
      "outputs": [],
      "source": [
        "plot_cm(cm_val, \"Confusion Matrix — Validation\")\n",
        "plot_cm(cm_te,  \"Confusion Matrix — Test\")\n",
        "print(f\"[Sanity] Val CM sum: {int(cm_val.sum())} | n_val: {len(y_true_val)}\")\n",
        "print(f\"[Sanity] Test CM sum: {int(cm_te.sum())} | n_test: {len(y_true_te)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCzHc0o3tphh"
      },
      "outputs": [],
      "source": [
        "def plot_roc(y_true, y_prob, title):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_prob); roc_auc = auc(fpr, tpr)\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.plot(fpr, tpr, label=f\"AUC={roc_auc:.3f}\")\n",
        "    plt.plot([0,1],[0,1],\"--\")\n",
        "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(title)\n",
        "    plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8jc16fqtsR_"
      },
      "outputs": [],
      "source": [
        "plot_roc(y_true_val, y_prob_val, \"ROC — Validation (TTA)\")\n",
        "plot_roc(y_true_te,  y_prob_te,  \"ROC — Test (TTA)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQBGJ9YQ7e41"
      },
      "outputs": [],
      "source": [
        "backbone = model.get_layer(\"efficientnetb0\")\n",
        "head = keras.Model(inputs=backbone.output, outputs=model.output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpdX6IPWnjdm"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def gradcam(img_batch):\n",
        "    pre = tf.keras.applications.efficientnet.preprocess_input(img_batch)\n",
        "    with tf.GradientTape() as tape:\n",
        "        feats = backbone(pre, training=False)      # (1, Hf, Wf, C)\n",
        "        preds = head(feats, training=False)        # (1, 1)\n",
        "        score = preds[:, 0]\n",
        "        tape.watch(feats)\n",
        "        grads = tape.gradient(score, feats)\n",
        "    weights = tf.reduce_mean(grads, axis=(1,2), keepdims=False)   # (1,C)\n",
        "    feats  = feats[0]\n",
        "    w      = weights[0]\n",
        "    cam    = tf.tensordot(feats, w, axes=[2,0])\n",
        "    cam    = tf.maximum(cam, 0); cam /= (tf.reduce_max(cam)+1e-12)\n",
        "    return cam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IF9cQqFG7-U3"
      },
      "outputs": [],
      "source": [
        "def overlay(img, heatmap, alpha=0.4):\n",
        "    if img.max() <= 1.0: img = (img*255.0).astype(\"uint8\")\n",
        "    heatmap_r = tf.image.resize(heatmap[..., tf.newaxis], img.shape[:2]).numpy().squeeze()\n",
        "    heatmap_rgb = plt.cm.jet(heatmap_r)[:, :, :3]; overlay = (heatmap_rgb*255).astype(\"uint8\")\n",
        "    return (alpha*overlay + (1-alpha)*img).astype(\"uint8\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIy0siLc8DH_"
      },
      "outputs": [],
      "source": [
        "print(\"\\nGrad-CAM examples:\")\n",
        "for idx, (img, lab) in enumerate(test_ds.unbatch().take(3), 1):\n",
        "    cam = gradcam(tf.expand_dims(img, 0)).numpy()\n",
        "    blended = overlay(img.numpy(), cam, 0.45)\n",
        "    plt.figure(figsize=(8,3))\n",
        "    plt.subplot(1,3,1); plt.imshow((img.numpy()*255).astype(\"uint8\")); plt.title(\"Input\"); plt.axis(\"off\")\n",
        "    plt.subplot(1,3,2); plt.imshow(cam, cmap=\"jet\"); plt.title(\"Grad-CAM\"); plt.axis(\"off\")\n",
        "    plt.subplot(1,3,3); plt.imshow(blended); plt.title(\"Overlay\"); plt.axis(\"off\")\n",
        "    plt.suptitle(f\"Example {idx} — True: {classes[int(lab.numpy())]}\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GaAzWuM8kLI"
      },
      "outputs": [],
      "source": [
        "def model_predict_for_lime(imgs_np):\n",
        "    x = imgs_np.astype(\"float32\")           # 0..255 expected\n",
        "    # LIME sends uint8; our model pipeline expects 0..255 then preprocess inside graph\n",
        "    return np.hstack([1.0 - model.predict(x, verbose=0), model.predict(x, verbose=0)])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93-0RIwP8qrl"
      },
      "outputs": [],
      "source": [
        "from lime import lime_image\n",
        "from skimage.segmentation import slic\n",
        "\n",
        "sample = next(iter(test_ds.unbatch().take(1)))\n",
        "img0, label0 = sample\n",
        "img0_uint8 = (img0.numpy()*255).astype(\"uint8\")\n",
        "explainer = lime_image.LimeImageExplainer()\n",
        "explanation = explainer.explain_instance(\n",
        "    image=img0_uint8,\n",
        "    classifier_fn=model_predict_for_lime,\n",
        "    top_labels=2,\n",
        "    hide_color=0,\n",
        "    num_samples=1000,\n",
        "    segmentation_fn=lambda x: slic(x, n_segments=80, compactness=10, sigma=1, start_label=0)\n",
        ")\n",
        "pred_label = explanation.top_labels[0]\n",
        "lime_img, lime_mask = explanation.get_image_and_mask(\n",
        "    label=pred_label, positive_only=True, num_features=10, hide_rest=False\n",
        ")\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1); plt.imshow(img0_uint8); plt.title(f\"Original (True: {classes[int(label0.numpy())]})\"); plt.axis(\"off\")\n",
        "plt.subplot(1,2,2); plt.imshow(lime_img); plt.title(f\"LIME — regions for class {pred_label}\"); plt.axis(\"off\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "print(\"\\nClinical note: We tuned for higher sensitivity (Fβ>1). In screening, missing tumors (FN) is worse than over-calling (FP),\")\n",
        "print(\"so we 1) optimized threshold on Validation for Fβ, 2) used class weights, 3) added TTA to stabilize predictions.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zz94A8mesB7P"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}